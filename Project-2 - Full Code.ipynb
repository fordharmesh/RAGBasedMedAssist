{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project 2** Dharmesh Shah - MSFT-AI-Professional Program - Dec 24"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Business Use Case**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem Statement:**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the fast-paced environment of healthcare centers, healthcare professionals often face the challenge of quick and accurate diagnosis of patients while managing an ever-increasing volume of medical information. Ensuring that healthcare providers have access to the latest and most comprehensive medical knowledge is crucial for improving patient outcomes and reducing the time needed to make informed decisions.\n",
        "\n",
        "There are multiple challenges that these professionals encounter daily, a few being\n",
        "\n",
        "- Information Overload: Medical professionals need to go through vast amounts of data and research to make accurate diagnoses and treatment plans. This can be overwhelming and time-consuming.\n",
        "- Efficiency: For overall patient care and quality health outcomes, quick and accurate diagnosis is vital, especially in emergency situations.\n",
        "- Access to Trusted Knowledge: In the ever-evolving healthcare industry, providing access to reliable and up-to-date medical information from renowned manuals and research papers is essential for maintaining high standards of care.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Objective:**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "A renowned chain of hospitals has decided to leverage AI to build a state-of-the-art solution to help healthcare professionals overcome the aforementioned challenges. They have recruited you as an AI specialist and tasked you with building a RAG-based AI solution that leverages renowned medical manuals as its knowledge base. This AI system will act as a POC towards an end product that’ll assist healthcare professionals in making better, quicker, and more accurate diagnoses, ultimately leading to faster patient resolutions and enabling better patient outcomes by reducing errors in diagnosis, saving valuable time for information retrieval, and standardizing care practices across the board."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Questions:**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Diagnostic Assistance**: \"What are the common symptoms and treatments for pulmonary embolism?\"\n",
        "\n",
        "**2. Drug Information**: \"Can you provide the trade names of medications used for treating hypertension?\"\n",
        "\n",
        "**3. Treatment Plans**: \"What are the first-line options and alternatives for managing rheumatoid arthritis?\"\n",
        "\n",
        "**4. Specialty Knowledge**: \"What are the diagnostic steps for suspected endocrine disorders?\"\n",
        "\n",
        "**5. Critical Care Protocols**: \"What is the protocol for managing sepsis in a critical care unit?\""
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **How This Application Empowers Professionals and Elevates Healthcare Organizations**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Enhanced Diagnostic Accuracy:**\n",
        "Provides evidence-based insights for better diagnosis and treatment.\n",
        "\n",
        "- **Time Efficiency:**\n",
        "Instantly retrieves critical information, saving valuable time.\n",
        "\n",
        "- **Improved Patient Care:**\n",
        "Ensures informed decisions with up-to-date medical knowledge.\n",
        "\n",
        "- **Cost-Effective Operations:**\n",
        "Reduces redundant tests and consultation delays, lowering costs.\n",
        "\n",
        "- **Knowledge Empowerment:**\n",
        "Keeps doctors updated on the latest advancements.\n",
        "\n",
        "- **Competitive Edge for the Hospital:**\n",
        "Positions the hospital as a leader in healthcare innovation.\n",
        "\n",
        "This collaboration between St. Bernard’s Medical Center and InnoviTech Solutions highlights the transformative potential of AI in revolutionizing healthcare."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Install and Import Required Libraries**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Azure Machine Learning SDK and FAISS-related utilities\n",
        "%pip install azure-ai-ml\n",
        "%pip install -U 'azureml-rag[faiss,hugging_face]>=0.2.36'\n",
        "%pip install azure-identity"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1745200113549
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Configure Azure Machine Learning Workspace**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get client for AzureML Workspace"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Write you are code here (AzureML and authentication libraries)\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient\n",
        "from azureml.core import Workspace"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1745197947511
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define workspace configuration (replace with your details)\n",
        "workspace_config = {\n",
        "    \"subscription_id\": \"d8d51058-2fe2-4f6a-bf7e-c2f2328a6998\",\n",
        "    \"resource_group\": \"GL-Dec24-Pro\", \n",
        "    \"workspace_name\": \"FitwellWorkspace\"\n",
        "}"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1745197949545
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile workspace.json\n",
        "{\n",
        "    \"subscription_id\": \"d8d51058-2fe2-4f6a-bf7e-c2f2328a6998\",\n",
        "    \"resource_group\": \"GL-Dec24-Pro\", \n",
        "    \"workspace_name\": \"FitwellWorkspace\"\n",
        "}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting workspace.json\n"
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize credentials for Azure authentication\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1745197956917
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the MLClient to connect with AzureML\n",
        "ml_client = MLClient.from_config(credential=credential, path=\"workspace.json\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found the config file in: workspace.json\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1745197961548
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an AzureML Workspace object\n",
        "ws = Workspace(\n",
        "    subscription_id=ml_client.subscription_id,\n",
        "    resource_group=ml_client.resource_group_name,\n",
        "    workspace_name=ml_client.workspace_name,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1745197961954
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## **3. Register the Reports Dataset as a Data Asset**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Write you are code here\n",
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the ZIP file containing Tesla annual reports\n",
        "zip_file_path = 'MedicalDiagnosisManuals.zip'\n",
        "\n",
        "# Directory to extract the reports\n",
        "extract_to_directory = './extracted_dataset_reports'\n",
        "os.makedirs(extract_to_directory, exist_ok=True)\n",
        "\n",
        "# Extract the ZIP file containing the reports\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to_directory)\n",
        "\n",
        "# Register the extracted data as a Data asset in AzureML\n",
        "local_data_path = extract_to_directory\n",
        "data_asset_name = \"Bernards-medical-manuals\"\n",
        "data_asset_description = \"A collection of medical manuals used by St. Bernard's Medical Center for embedding generation and knowledge retrieval in the RAG system.\"\n",
        "\n",
        "data_asset = Data(\n",
        "    path=local_data_path,\n",
        "    type=AssetTypes.URI_FOLDER,  # Registering as a folder URI (AssetTypes.URI_FOLDER)\n",
        "    description=data_asset_description,\n",
        "    name=data_asset_name\n",
        ")\n",
        "\n",
        "# Use the MLClient to register the data asset\n",
        "ml_client.data.create_or_update(data_asset)\n",
        "print(f\"Data asset '{data_asset.name}' registered successfully.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Data asset 'Bernards-medical-manuals' registered successfully.\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1745197968010
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Set Up Azure OpenAI Connection**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the cells under _either_ heading (OpenAI or HuggingFace) to use the respective embedding model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Note:**\n",
        "\n",
        "When selecting an embedding model, ensure consistency between the embedding model used here and the one configured for your vectorstore retriever. Using mismatched models may result in dimension mismatches, leading to errors during vector retrieval. For example:\n",
        "\n",
        "- If you use the `text-embedding-ada-002` model from Azure OpenAI for embedding creation, ensure the same model is specified for the vectorstore retriever.\n",
        "- Similarly, if using a HuggingFace model like `all-mpnet-base-v2`, configure the vectorstore retriever with this model.\n",
        "\n",
        "Maintaining alignment between the embedding model and vectorstore retriever is crucial for the proper functioning of your pipeline."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OpenAI"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# # Azure Open AI redentials and the id of the deployed chat model are stored as\n",
        "# # key value pairs in a json file\n",
        "with open('config.json', 'r') as az_creds: #Fill the blank with json credentails file \n",
        "    data = az_creds.read()\n",
        "\n",
        "# Credentials to authenticate to the personalized Open AI model server\n",
        "import json\n",
        "creds = json.loads(data)"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1745197989397
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.rag.utils.connections import get_connection_by_name_v2, create_connection_v2\n",
        "\n",
        "# Define the connection name for Azure OpenAI\n",
        "aoai_connection_name = \"MLS13_December24\"\n",
        "\n",
        "# If the connection doesn't exist, create a new one\n",
        "target = creds[\"AZURE_OPENAI_ENDPOINT\"]  # Replace with your Azure OpenAI endpoint\n",
        "key = creds[\"AZURE_OPENAI_KEY\"]          # Replace with your Azure OpenAI API key\n",
        "api_version = creds[\"AZURE_OPENAI_APIVERSION\"]    # Replace with the appropriate API version\n",
        "\n",
        "aoai_connection = create_connection_v2(\n",
        "    workspace=ws,\n",
        "    name=aoai_connection_name,\n",
        "    category=\"AzureOpenAI\",\n",
        "    target=target,\n",
        "    auth_type=\"ApiKey\",\n",
        "    credentials={\"key\": key},\n",
        "    metadata={\"ApiType\": \"azure\", \"ApiVersion\": api_version},\n",
        ")\n",
        "\n",
        "aoai_connection_id = aoai_connection[\"id\"]\n",
        "\n",
        "print(f\"Azure OpenAI connection created or retrieved successfully: {aoai_connection_id}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Azure OpenAI connection created or retrieved successfully: /subscriptions/d8d51058-2fe2-4f6a-bf7e-c2f2328a6998/resourceGroups/GL-Dec24-Pro/providers/Microsoft.MachineLearningServices/workspaces/FitwellWorkspace/connections/MLS13_December24\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1745198003801
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the Azure OpenAI `text-embedding-ada-002` model for embedding generation, ensure the model has been deployed and is ready for inference. [Follow these instructions](https://learn.microsoft.com/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#deploy-a-model) to deploy the embedding model in your Azure OpenAI resource.\n",
        "\n",
        "Once deployed, obtain the credentials for the text-embedding-ada-002 model and save them in your `config.json` file. This is necessary for the following code to execute successfully."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensure that the `config.json` file includes the appropriate keys:\n",
        "\n",
        "- `AZURE_OPENAI_EMBEDDING_MODEL`: Name of the Azure OpenAI embedding model.\n",
        "- `AZURE_OPENAI_EMBEDING_DEPLOYMENT`: Deployment name for the embedding model.\n",
        "\n",
        "Finally we will combine the deployment and model information into a uri form which the AzureML embeddings components expect as input."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "aoai_embedding_model_name = creds[\"AZURE_OPENAI_EMBEDDING_MODEL\"]\n",
        "aoai_embedding_deployment_name = creds[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"]\n",
        "embeddings_model_uri = f\"azure_open_ai://deployment/{aoai_embedding_deployment_name}/model/{aoai_embedding_model_name}\"\n",
        "print(f\"Embedding Model URI: {embeddings_model_uri}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Embedding Model URI: azure_open_ai://deployment/text-embedding-ada-002/model/text-embedding-ada-002\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1745198015272
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### HuggingFace\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#embeddings_model_uri = \"hugging_face://model/sentence-transformers/_________\"   #Fill the blank with embedding model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1732711996834
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## **5. Setup Pipeline to process data into Index**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Define Pipeline Components**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Write you are code here\n",
        "# Import the MLClient to access the AzureML registry\n",
        "ml_registry = MLClient(credential=credential, registry_name=\"azureml\")\n",
        "\n",
        "# Retrieve components for processing data, generating embeddings, and creating the FAISS index\n",
        "crack_and_chunk_component = ml_registry.components.get(\n",
        "    \"llm_rag_crack_and_chunk\", label=\"latest\"\n",
        ")\n",
        "generate_embeddings_component = ml_registry.components.get(\n",
        "    \"llm_rag_generate_embeddings\", label=\"latest\"\n",
        ")\n",
        "create_faiss_index_component = ml_registry.components.get(\n",
        "    \"llm_rag_create_faiss_index\", label=\"latest\"\n",
        ")\n",
        "register_mlindex_component = ml_registry.components.get(\n",
        "    \"llm_rag_register_mlindex_asset\", label=\"latest\"\n",
        ")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Overriding of current TracerProvider is not allowed\nOverriding of current LoggerProvider is not allowed\nOverriding of current MeterProvider is not allowed\nAttempting to instrument while already instrumented\nAttempting to instrument while already instrumented\nAttempting to instrument while already instrumented\nAttempting to instrument while already instrumented\nAttempting to instrument while already instrumented\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1745198031429
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(crack_and_chunk_component)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\nname: llm_rag_crack_and_chunk\nversion: 0.0.78\ndisplay_name: LLM - Crack and Chunk Data\ndescription: 'Creates chunks no larger than `chunk_size` from `input_data`, extracted\n  document titles are prepended to each chunk\n\n\n  LLM models have token limits for the prompts passed to them, this is a limiting\n  factor at embedding time and even more limiting at prompt completion time as only\n  so much context can be passed along with instructions to the LLM and user queries.\n\n  Chunking allows splitting source data of various formats into small but coherent\n  snippets of information which can be ''packed'' into LLM prompts when asking for\n  answers to user query related to the source documents.\n\n\n  Supported formats: md, txt, html/htm, pdf, ppt(x), doc(x), xls(x), py\n\n  '\ntags:\n  Preview: ''\ntype: command\ninputs:\n  input_data:\n    type: uri_folder\n    description: Uri Folder containing files to be chunked.\n    optional: false\n  input_glob:\n    type: string\n    optional: true\n    description: Limit files opened from `input_data`, defaults to '**/*'.\n  allowed_extensions:\n    type: string\n    optional: true\n    description: Comma separated list of extensions to include, if not provided the\n      default list of supported extensions will be used. e.g. '.md,.txt,.html,.py,.pdf.'\n  chunk_size:\n    type: integer\n    optional: false\n    default: '768'\n    description: Maximum number of tokens to put in each chunk.\n  chunk_overlap:\n    type: integer\n    optional: false\n    default: '0'\n    description: Number of tokens to overlap between chunks.\n  doc_intel_connection_id:\n    type: string\n    optional: true\n    description: Connection id for Document Intelligence service. If provided, will\n      be used to extract content from .pdf document.\n  data_source_url:\n    type: string\n    optional: true\n    description: Base URL to join with file paths to create full source file URL for\n      chunk metadata.\n  document_path_replacement_regex:\n    type: string\n    optional: true\n    description: 'A JSON string with two fields, ''match_pattern'' and ''replacement_pattern''\n      to be used with re.sub on the source url. e.g. ''{\"match_pattern\": \"(.*)/articles/(.*)(\\\\.[^.]+)$\",\n      \"replacement_pattern\": \"\\\\1/\\\\2\"}'' would remove ''/articles'' from the middle\n      of the url.'\n  max_sample_files:\n    type: integer\n    optional: false\n    default: '-1'\n    description: Number of files to chunk. Specify -1 to chunk all documents in input\n      path.\n  use_rcts:\n    type: string\n    optional: false\n    default: 'True'\n    description: Whether to use RecursiveCharacterTextSplitter to split documents\n      into chunks\n    enum:\n    - 'True'\n    - 'False'\n  output_format:\n    type: string\n    optional: false\n    default: jsonl\n    description: Format of the output chunk file\n    enum:\n    - csv\n    - jsonl\noutputs:\n  output_chunks:\n    type: uri_folder\n    description: Uri Folder containing chunks. Each chunk will be a separate file\n      in the folder\ncommand: python -m azureml.rag.tasks.crack_and_chunk --input_data '${{inputs.input_data}}'\n  $[[--input_glob '${{inputs.input_glob}}']] $[[--allowed_extensions ${{inputs.allowed_extensions}}]]\n  --output_chunks ${{outputs.output_chunks}} --chunk_size ${{inputs.chunk_size}} --chunk_overlap\n  ${{inputs.chunk_overlap}} $[[--doc_intel_connection_id ${{inputs.doc_intel_connection_id}}]]\n  $[[--data_source_url ${{inputs.data_source_url}}]] $[[--document_path_replacement_regex\n  '${{inputs.document_path_replacement_regex}}']] --max_sample_files ${{inputs.max_sample_files}}\n  --use_rcts '${{inputs.use_rcts}}' --output_format ${{inputs.output_format}}\nenvironment: azureml://registries/azureml/environments/llm-rag-embeddings/versions/76\ncode: azureml://registries/azureml/codes/2062c1ba-8a97-4ea3-acc3-a7b2e5f8fa58/versions/1\nresources:\n  instance_count: 1\ncreation_context:\n  created_at: '2025-01-02T05:48:55.228512+00:00'\n  created_by: Microsoft\n  created_by_type: User\n  last_modified_at: '2025-01-02T05:48:55.228512+00:00'\n  last_modified_by: Microsoft\n  last_modified_by_type: User\nid: azureml://registries/azureml/components/llm_rag_crack_and_chunk/versions/0.0.78\nis_deterministic: true\n\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1745198031593
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Build the AzureML Pipeline**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Write you are code here\n",
        "from azure.ai.ml import Input, Output\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.ai.ml.entities._job.pipeline._io import PipelineInput\n",
        "from typing import Optional"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1745198039135
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility function for automatic compute configuration\n",
        "def use_automatic_compute(component, instance_count=1, instance_type=\"Standard_NC4as_T4_v3\"):\n",
        "    \"\"\"Configure a component to use automatic compute.\"\"\"\n",
        "    component.set_resources(\n",
        "        instance_count=instance_count,\n",
        "        instance_type=instance_type,\n",
        "        properties={\"compute_specification\": {\"automatic\": True}},\n",
        "    )\n",
        "    return component\n",
        "\n",
        "\n",
        "# Utility function to check if optional pipeline inputs are provided\n",
        "def optional_pipeline_input_provided(input: Optional[PipelineInput]):\n",
        "    \"\"\"Check if optional pipeline inputs are provided.\"\"\"\n",
        "    return input is not None and input._data is not None    "
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1745198048845
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@pipeline(default_compute=\"serverless\")\n",
        "def diagnosismanuals_to_faiss(\n",
        "    data_asset_path: str,\n",
        "    embeddings_model: str,\n",
        "    asset_name: str,\n",
        "    chunk_size: int = 1024,\n",
        "    data_source_glob: str = None,\n",
        "    document_path_replacement_regex: str = None,\n",
        "    aoai_connection_id=None,\n",
        "    embeddings_container=None,\n",
        "):\n",
        "    \"\"\"Pipeline to process Bernards-medical-manuals and create a FAISS vector index.\"\"\"\n",
        "    \n",
        "    # Step 1: Chunk data into smaller pieces\n",
        "    crack_and_chunk = crack_and_chunk_component(\n",
        "        input_data=Input(type=\"uri_folder\", path=data_asset_path),  # Input data asset\n",
        "        input_glob=data_source_glob,\n",
        "        chunk_size=chunk_size,\n",
        "        document_path_replacement_regex=document_path_replacement_regex,\n",
        "    )\n",
        "    use_automatic_compute(crack_and_chunk)  # Apply compute configuration\n",
        "\n",
        "    # Step 2: Generate embeddings for the data chunks\n",
        "    generate_embeddings = generate_embeddings_component(\n",
        "        chunks_source=crack_and_chunk.outputs.output_chunks,\n",
        "        embeddings_container=embeddings_container,\n",
        "        embeddings_model=embeddings_model,\n",
        "    )\n",
        "    use_automatic_compute(generate_embeddings)  # Apply compute configuration\n",
        "    \n",
        "    # Optional: Include Azure OpenAI connection ID\n",
        "    if optional_pipeline_input_provided(aoai_connection_id):\n",
        "        generate_embeddings.environment_variables[\n",
        "            \"AZUREML_WORKSPACE_CONNECTION_ID_AOAI\"\n",
        "        ] = aoai_connection_id\n",
        "    \n",
        "    if optional_pipeline_input_provided(embeddings_container):\n",
        "        generate_embeddings.outputs.embeddings = Output(\n",
        "            type=\"uri_folder\", path=f\"{embeddings_container.path}/{{name}}\"\n",
        "        )\n",
        "\n",
        "    # Step 3: Create a FAISS vector index from embeddings\n",
        "    create_faiss_index = create_faiss_index_component(\n",
        "        embeddings=generate_embeddings.outputs.embeddings,\n",
        "    )\n",
        "    use_automatic_compute(create_faiss_index)  # Apply compute configuration\n",
        "\n",
        "    # Step 4: Register the FAISS index as an MLIndex asset\n",
        "    register_mlindex = register_mlindex_component(\n",
        "        storage_uri=create_faiss_index.outputs.index, \n",
        "        asset_name=asset_name\n",
        "    )\n",
        "    use_automatic_compute(register_mlindex) # Apply compute configuration\n",
        "    \n",
        "    return {\n",
        "        \"mlindex_asset_uri\": create_faiss_index.outputs.index,\n",
        "        \"mlindex_asset_id\": register_mlindex.outputs.asset_id,\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1745198160867
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.Submit the Pipeline**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the asset name and data source glob pattern\n",
        "# asset_name = \"diagnosis_faiss_index\"  #Name for the FAISS index asset created using Huggingface model embeddings.\n",
        "asset_name = \"diagnosis_faiss\"  #Name for the FAISS index asset created using OpenAI model embeddings.\n",
        "data_source_glob = \"**/*.pdf\"  # Pattern to match input data files"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1745198167553
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the input data asset path from the workspace datastore\n",
        "datastore_path = ml_client.data.get(data_asset_name, version=\"1\").path\n",
        "print(f\"Datastore path: {datastore_path}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Datastore path: azureml://subscriptions/d8d51058-2fe2-4f6a-bf7e-c2f2328a6998/resourcegroups/GL-Dec24-Pro/workspaces/FitwellWorkspace/datastores/workspaceblobstore/paths/LocalUpload/ab0555d33a743acbb9fc52dfbb7b50b1/extracted_dataset_reports/\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1745198170913
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the pipeline job by calling the defined pipeline function\n",
        "pipeline_job = diagnosismanuals_to_faiss(\n",
        "    embeddings_model=embeddings_model_uri,  # URI of the embeddings model\n",
        "    aoai_connection_id=aoai_connection_id,  # Connection ID for Azure OpenAI (optional)\n",
        "    embeddings_container=Input(\n",
        "        type=\"uri_folder\",\n",
        "        path=f\"azureml://datastores/workspaceblobstore/paths/embeddings/{asset_name}\"\n",
        "    ),  # Path for storing generated embeddings\n",
        "    data_asset_path=Input(\n",
        "        type=\"uri_folder\",\n",
        "        path=datastore_path\n",
        "    ),  # Input data asset path\n",
        "    chunk_size=1024,  # Size of chunks for processing\n",
        "    data_source_glob=data_source_glob,  # Glob pattern for input files\n",
        "    asset_name=asset_name,  # Name of the MLIndex asset\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1745198181755
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add properties for better indexing and artifact tracking in the AzureML UI\n",
        "pipeline_job.properties[\"azureml.mlIndexAssetName\"] = asset_name\n",
        "pipeline_job.properties[\"azureml.mlIndexAssetKind\"] = \"faiss\"\n",
        "pipeline_job.properties[\"azureml.mlIndexAssetSource\"] = \"Data asset\""
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1745198309435
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit the pipeline job for execution\n",
        "submitted_pipeline = ml_client.jobs.create_or_update(pipeline_job)\n",
        "print(f\"Pipeline submitted successfully! Job ID: {submitted_pipeline.id}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\npathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\npathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\npathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFileJobOutput'> and will be ignored\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pipeline submitted successfully! Job ID: /subscriptions/d8d51058-2fe2-4f6a-bf7e-c2f2328a6998/resourceGroups/GL-Dec24-Pro/providers/Microsoft.MachineLearningServices/workspaces/FitwellWorkspace/jobs/olive_muscle_fj1n540srv\n"
        }
      ],
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1745198313984
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stream the pipeline job logs for real-time monitoring\n",
        "ml_client.jobs.stream(submitted_pipeline.name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RunId: olive_muscle_fj1n540srv\nWeb View: https://ml.azure.com/runs/olive_muscle_fj1n540srv?wsid=/subscriptions/d8d51058-2fe2-4f6a-bf7e-c2f2328a6998/resourcegroups/GL-Dec24-Pro/workspaces/FitwellWorkspace\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2025-04-21 01:18:26Z] Completing processing run id cb672b76-bebb-4d05-9ff5-5cd9ce06541e.\n[2025-04-21 01:18:27Z] Completing processing run id fdd51e1a-66d0-45d3-a6b8-8e08f74c0054.\n[2025-04-21 01:18:28Z] Completing processing run id ee727436-33d2-4cf9-a0ad-ca3a48cf1f85.\n[2025-04-21 01:18:29Z] Completing processing run id 53dc3b48-acf4-46d9-be0f-154298413d23.\n[2025-04-21 01:18:30Z] Finishing experiment: no runs left and nothing to schedule.\n\nExecution Summary\n=================\nRunId: olive_muscle_fj1n540srv\nWeb View: https://ml.azure.com/runs/olive_muscle_fj1n540srv?wsid=/subscriptions/d8d51058-2fe2-4f6a-bf7e-c2f2328a6998/resourcegroups/GL-Dec24-Pro/workspaces/FitwellWorkspace\n\n"
        }
      ],
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1745198323840
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.Execution Pipeline**\n",
        "\n",
        "[Jobs](https://huggingface.co/spaces/fordharmesh/Intelligent_reporting_on_azure/resolve/main/RAGProjectExecutionPiplelineList.jpg)\n",
        "\n",
        "![Jobs](https://huggingface.co/spaces/fordharmesh/Intelligent_reporting_on_azure/resolve/main/RAGProjectExecutionPiplelineList.jpg)\n",
        "\n",
        "\n",
        "[Pipeline](https://huggingface.co/spaces/fordharmesh/Intelligent_reporting_on_azure/resolve/main/RAGProjectExecutionPipleline.jpg)\n",
        "\n",
        "![Pipeline](https://huggingface.co/spaces/fordharmesh/Intelligent_reporting_on_azure/resolve/main/RAGProjectExecutionPipleline.jpg)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Information Retrieval and Response Generation Using LangChain-FAISS and Azure OpenAI**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.Installing Required Libraries**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required LangChain and HuggingFace libraries\n",
        "%pip install -U langchain-community\n",
        "%pip install -U langchain-huggingface\n",
        "%pip install -U langchain-openai"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: langchain-community in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (0.3.21)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-community) (0.3.54)\nRequirement already satisfied: langchain<1.0.0,>=0.3.23 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-community) (0.3.23)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-community) (2.0.40)\nRequirement already satisfied: requests<3,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-community) (2.32.3)\nRequirement already satisfied: PyYAML>=5.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-community) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-community) (3.11.17)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-community) (9.1.2)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-community) (0.6.7)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-community) (2.9.1)\nRequirement already satisfied: langsmith<0.4,>=0.1.125 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-community) (0.3.32)\nRequirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-community) (0.4.0)\nRequirement already satisfied: numpy<3,>=1.26.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-community) (2.2.5)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (0.3.8)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (2.11.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (4.13.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\nRequirement already satisfied: python-dotenv>=0.21.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\nRequirement already satisfied: typing-inspection>=0.4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2025.1.31)\nRequirement already satisfied: greenlet>=1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\nRequirement already satisfied: anyio in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\nRequirement already satisfied: httpcore==1.* in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.8)\nRequirement already satisfied: h11<0.15,>=0.13 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (2.33.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.2.2)\nRequirement already satisfied: sniffio>=1.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: langchain-huggingface in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-huggingface) (0.30.2)\nRequirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-huggingface) (0.3.54)\nRequirement already satisfied: sentence-transformers>=2.6.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-huggingface) (4.1.0)\nRequirement already satisfied: tokenizers>=0.19.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-huggingface) (0.21.1)\nRequirement already satisfied: transformers>=4.39.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-huggingface) (4.51.3)\nRequirement already satisfied: filelock in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2023.10.0)\nRequirement already satisfied: packaging>=20.9 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (6.0.2)\nRequirement already satisfied: requests in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.13.0)\nRequirement already satisfied: langsmith<0.4,>=0.1.125 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.3.32)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (9.1.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.33)\nRequirement already satisfied: pydantic<3.0.0,>=2.5.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.11.2)\nRequirement already satisfied: torch>=1.11.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.6.0)\nRequirement already satisfied: scikit-learn in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.6.1)\nRequirement already satisfied: scipy in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.15.2)\nRequirement already satisfied: Pillow in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (11.1.0)\nRequirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers>=4.39.0->langchain-huggingface) (2.2.5)\nRequirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers>=4.39.0->langchain-huggingface) (2024.11.6)\nRequirement already satisfied: safetensors>=0.4.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers>=4.39.0->langchain-huggingface) (0.5.3)\nRequirement already satisfied: jsonpointer>=1.9 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.0.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.10.16)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.23.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2025.1.31)\nRequirement already satisfied: networkx in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.4.2)\nRequirement already satisfied: jinja2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\nRequirement already satisfied: joblib>=1.2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.6.0)\nRequirement already satisfied: anyio in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (4.9.0)\nRequirement already satisfied: httpcore==1.* in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.8)\nRequirement already satisfied: h11<0.15,>=0.13 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.14.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.0.2)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.2.2)\nRequirement already satisfied: sniffio>=1.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.3.1)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: langchain-openai in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (0.3.14)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.53 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-openai) (0.3.54)\nRequirement already satisfied: openai<2.0.0,>=1.68.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-openai) (1.75.0)\nRequirement already satisfied: tiktoken<1,>=0.7 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-openai) (0.9.0)\nRequirement already satisfied: langsmith<0.4,>=0.1.125 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.3.32)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (9.1.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (1.33)\nRequirement already satisfied: PyYAML>=5.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (6.0.2)\nRequirement already satisfied: packaging<25,>=23.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (4.13.0)\nRequirement already satisfied: pydantic<3.0.0,>=2.5.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (2.11.2)\nRequirement already satisfied: anyio<5,>=3.5.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\nRequirement already satisfied: sniffio in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\nRequirement already satisfied: regex>=2022.1.18 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\nRequirement already satisfied: requests>=2.26.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai) (1.2.2)\nRequirement already satisfied: idna>=2.8 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai) (3.10)\nRequirement already satisfied: certifi in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (1.0.8)\nRequirement already satisfied: h11<0.15,>=0.13 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.53->langchain-openai) (3.0.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain-openai) (3.10.16)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain-openai) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.23.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain-openai) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1745198344867
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Setting Up Data Retrieval**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Downloading and Setting Up FAISS Index Assets**\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary utilities for artifact retrieval\n",
        "import azure.ai.ml._artifacts._artifact_utilities as artifact_utils\n",
        "\n",
        "# Retrieve the path to the latest FAISS index asset from Azure ML\n",
        "data_info = ml_client.data.get(name=asset_name, label=\"latest\").path\n",
        "\n",
        "# Download the FAISS index asset to a local directory\n",
        "artifact_utils.download_artifact_from_aml_uri(\n",
        "    uri=data_info,\n",
        "    destination=\"./diagnosisfaissindexasset/\",\n",
        "    datastore_operation=ml_client.datastores\n",
        ")\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 31,
          "data": {
            "text/plain": "'./diagnosisfaissindexasset/'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 31,
      "metadata": {
        "gather": {
          "logged": 1745198428076
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Loading the FAISS Index**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading the FAISS Index and Preparing the Retriever**\n",
        "\n",
        "We load the FAISS index from the downloaded files and connect it to an embedding model. This embedding model ensures that queries are converted into vector space to match the stored documents effectively."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Note:**\n",
        "When loading the FAISS index and setting up the retriever, it is crucial to use the same embedding model that was used during the creation of the FAISS index. This ensures that the dimensionality of the embeddings produced by the query matches the dimensionality of the embeddings stored in the FAISS index.\n",
        "\n",
        "If a different embedding model is used between these two steps, a dimension mismatch will occur, leading to errors in retrieving relevant documents or performing similarity searches."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the directory containing FAISS index files\n",
        "index_folder_path = \"./diagnosisfaissindexasset/\""
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "gather": {
          "logged": 1745198455247
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from langchain_huggingface import HuggingFaceEmbeddings\n",
        "#\n",
        "## Specify the embedding model used during FAISS index creation\n",
        "#embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "#embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_openai import AzureOpenAIEmbeddings\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "\n",
        "# # Specify the embedding model used during FAISS index creation\n",
        "embedding_model = AzureOpenAIEmbeddings(\n",
        "    model=creds[\"AZURE_OPENAI_EMBEDDING_MODEL\"],\n",
        "    azure_endpoint= creds[\"AZURE_OPENAI_ENDPOINT\"],\n",
        "    api_key= creds[\"AZURE_OPENAI_KEY\"],\n",
        "    openai_api_version=creds[\"AZURE_OPENAI_APIVERSION\"]\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 34,
      "metadata": {
        "gather": {
          "logged": 1745198503390
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Load the FAISS index and associate it with the embedding model\n",
        "retriever = FAISS.load_local(\n",
        "    folder_path=index_folder_path, \n",
        "    embeddings=embedding_model, \n",
        "    allow_dangerous_deserialization=True  # Acknowledge the source of the data for safe loading\n",
        ")\n",
        "\n",
        "# The retriever is now ready to perform similarity searches."
      ],
      "outputs": [],
      "execution_count": 35,
      "metadata": {
        "gather": {
          "logged": 1745198513170
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Performing a Similarity Search**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a query to test the retriever\n",
        "query = \"How to diagnos Nutritional Disorders\"\n",
        "\n",
        "# Retrieve the top 3 most relevant documents\n",
        "results = retriever.similarity_search(query, k=3)\n",
        "\n",
        "# Display the results\n",
        "for doc in results:\n",
        "    print(f\"Document: {doc.page_content}\\nMetadata: {doc.metadata}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Document: Title: The_Merck_Manual_of_Diagnosis_and_Therapy_2011 - 19th Edn........pdfDiagnosis is based on results of medical and diet histories, physical examination, body composition\nanalysis (see p. \n58\n), and selected laboratory tests.\nHistory:\n History should include questions about dietary intake (see\nFig. 2-1\n), recent changes in weight, and risk factors for undernutrition, including drug and alcohol use.\nUnintentional loss of ≥ 10% of usual body weight during a 3-mo period indicates a high probability of\nundernutrition. Social history should include questions about whether money is available for food and\nwhether the patient can shop and cook.\nReview of systems should focus on symptoms of nutritional deficiencies (see \nTable 2-1\n). For example,\nimpaired night vision may indicate vitamin A deficiency.\nPhysical examination:\n Physical examination should include measurement of height and weight,\ninspection of body fat distribution, and anthropometric measurements of lean body mass. Body mass\nindex (BMI = weight(kg)/height(m)\n2\n) adjusts weight for height (see\nTable 6-2\n on p. \n59\n). If weight is < 80% of what is predicted for the patient's height or if BMI is ≤ 18,\nundernutrition should be suspected. Although these findings are useful in diagnosing undernutrition and\nare acceptably sensitive, they lack specificity.\n[\nFig. 2-1.\n Mini nutritional assessment.]\nThe mid upper arm muscle area estimates lean body mass. This area is derived from the triceps skinfold\nthickness (TSF) and mid upper arm circumference. Both are measured at the same site, with the patient's\nright arm in a relaxed position. The average mid upper arm circumference is about 32 ± 5 cm for men and\n28 ± 6 cm for women. The formula for calculating the mid upper arm muscle area in cm\n2\n is as follows:\nThis formula corrects the upper arm area for fat and bone. Average values for the mid upper arm muscle\narea are 54 ± 11 cm\n2\n for men and 30 ± 7 cm\n2\n for women. A value < 75% of this standard (depending on\nage) indicates depletion of lean body mass (see\nTable 2-2\n). This measurement may be affected by physical activity, genetic factors, and age-related\nmuscle loss.\nPhysical examination should focus on signs of specific nutritional deficiencies. Signs of PEU (eg, edema,\nmuscle wasting, skin changes) should be sought. Examination should also focus on signs of conditions\nthat could predispose to nutritional deficiencies, such as dental problems. Mental status should be\nassessed, because depression and cognitive impairment can lead to weight loss.\nThe widely used Subjective Global Assessment (SGA) uses information from the patient history (eg,\nweight loss, change in intake, GI symptoms), physical examination findings (eg, loss of muscle and\nsubcutaneous fat, edema, ascites), and the clinician's judgment of the patient's nutritional status. The\nMini Nutritional Assessment (MNA) has been validated and is widely used, especially for elderly patients\n(see \nFig. 2-1\n). The Simplified Nutrition Assessment Questionnaire (SNAQ), a simple, validated method of\npredicting future weight loss, may be used (see\nFig. 2-2\n).\nTesting:\n The extent of laboratory testing needed is unclear and may depend on the patient's\ncircumstances. If the cause is obvious and correctable (eg, a wilderness survival situation), testing is\nprobably of little benefit. Other patients may require more detailed evaluation.\nSerum albumin measurement is the laboratory test most often used. Decreases in albumin and other\nproteins (eg, prealbumin [transthyretin], transferrin, retinol-binding protein) may indicate protein deficiency\nor PEU. As undernutrition progresses, albumin decreases slowly; prealbumin, transferrin, and retinol-\nbinding protein decrease rapidly. Albumin measurement is inexpensive and predicts morbidity andThe Merck Manual of Diagnosis & Therapy, 19th Edition\nChapter 2. Undernutrition\n61\nMetadata: {'source_doc_id': 'MedicalDiagnosisManuals/The_Merck_Manual_of_Diagnosis_and_Therapy_2011 - 19th Edn........pdf71', 'chunk_hash': '75d456751fe34a74efd0859e06055c30ce4e78c9e39319c19010cdaec8694d3a', 'mtime': None, 'page_number': 70, 'stats': {'tiktokens': 890, 'chars': 3882, 'lines': 70}, 'source': {'filename': 'MedicalDiagnosisManuals/The_Merck_Manual_of_Diagnosis_and_Therapy_2011 - 19th Edn........pdf', 'url': 'MedicalDiagnosisManuals/The_Merck_Manual_of_Diagnosis_and_Therapy_2011 - 19th Edn........pdf', 'mtime': 1745183731.0}}\nDocument: Title: The_Merck_Manual_of_Diagnosis_and_Therapy_2011 - 19th Edn........pdf1 - Nutritional Disorders\nChapter 1. Nutrition: General Considerations\nIntroduction\nNutrition is the science of food and its relationship to health. Nutrients are chemicals in foods that are\nused by the body for growth, maintenance, and energy. Nutrients that cannot be synthesized by the body\nand thus must be derived from the diet are considered essential. They include vitamins, minerals, some\namino acids, and some fatty acids. Nutrients that the body can synthesize from other compounds,\nalthough they may also be derived from the diet, are considered nonessential. Macronutrients are\nrequired by the body in relatively large amounts; micronutrients are needed in minute amounts.\nLack of nutrients can result in deficiency syndromes (eg, kwashiorkor, pellagra) or other disorders (see p.\n9\n). Excess intake of macronutrients can lead to obesity (see p. \n56\n) and related disorders; excess intake of\nmicro-nutrients can be toxic. Also, the balance of various types of nutrients, such as how much\nusaturated vs saturated fat is consumed, can influence the development of disorders.\nMacronutrients\nMacronutrients constitute the bulk of the diet and supply energy and many essential nutrients.\nCarbohydrates, proteins (including essential amino acids), fats (including essential fatty acids),\nmacrominerals, and water are macronutrients. Carbohydrates, fats, and proteins are interchangeable as\nsources of energy; fats yield 9 kcal/g (37.8 kJ/g); proteins and carbohydrates yield 4 kcal/g (16.8 kJ/g).\nCarbohydrates:\n Dietary carbohydrates are broken down into glucose and other monosaccharides.\nCarbohydrates increase blood glucose levels, supplying energy. Simple carbohydrates are composed of\nsmall molecules, generally monosaccharides or disaccharides, which increase blood glucose levels\nrapidly. Complex carbohydrates are composed of larger molecules, which are broken down into\nmonosaccharides. Complex carbohydrates increase blood glucose levels more slowly but for a longer\ntime. Glucose and sucrose are simple carbohydrates; starches and fiber are complex carbohydrates.\nThe glycemic index measures how rapidly consumption of a carbohydrate increases plasma glucose\nlevels. Values range from 1 (the slowest increase) to 100 (the fastest increase, equivalent to pure glucose\n—see\nTable 1-1\n). However, the actual rate of increase also depends on what foods are consumed with the\ncarbohydrate.\nCarbohydrates with a high glycemic index may increase plasma glucose to high levels rapidly. It is\nhypothesized that, as a result, insulin levels increase, inducing hypoglycemia and hunger, which tends to\nlead to consumption of excess calories and weight gain. Carbohydrates with a low glycemic index\nincrease plasma glucose levels slowly, resulting in lower postprandial insulin levels and less hunger,\nwhich probably makes consumption of excess calories less likely. These effects are predicted to result in\na more favorable lipid profile and a decreased risk of obesity, diabetes mellitus, and complications of\ndiabetes if present.\nProteins:\n Dietary proteins are broken down into peptides and amino acids. Proteins are required for\ntissue maintenance, replacement, function, and growth. However, if the body is not getting enough\ncalories from dietary sources or tissue stores (particularly of fat), protein may be used for energy.\nAs the body uses dietary protein for tissue production, there is a net gain of protein (positive nitrogen\nbalance). During catabolic\n[\nTable 1-1.\n Glycemic Index of Some Foods]\nstates (eg, starvation, infections, burns), more protein may be used (because body tissues are broken\ndown) than is absorbed, resulting in a net loss of protein (negative nitrogen balance). Nitrogen balance is\nbest determined by subtracting the amount of nitrogen excreted in urine and feces from the amount ofThe Merck Manual of Diagnosis & Therapy, 19th Edition\nChapter 1. Nutrition: General Considerations\n53\nMetadata: {'source_doc_id': 'MedicalDiagnosisManuals/The_Merck_Manual_of_Diagnosis_and_Therapy_2011 - 19th Edn........pdf62', 'chunk_hash': '8b115458e43f04254cd1abb02f6c9ff4f009e13e07c823e2550c6a440b11d08d', 'mtime': None, 'page_number': 62, 'stats': {'tiktokens': 877, 'chars': 4002, 'lines': 55}, 'source': {'filename': 'MedicalDiagnosisManuals/The_Merck_Manual_of_Diagnosis_and_Therapy_2011 - 19th Edn........pdf', 'url': 'MedicalDiagnosisManuals/The_Merck_Manual_of_Diagnosis_and_Therapy_2011 - 19th Edn........pdf', 'mtime': 1745183731.0}}\nDocument: Title: The_Merck_Manual_of_Diagnosis_and_Therapy_2011 - 19th Edn........pdfEvaluating general nutritional status includes history, physical examination, and sometimes tests. If\nundernutrition is suspected, laboratory tests (eg, albumin levels) and skin tests for delayed\nhypersensitivity may be done (see p.\n13\n). Body composition analysis (eg, skinfold measurements, bioelectrical impedance analysis) is used to\nestimate percentage of body fat and to evaluate obesity (see p. \n58\n).\nHistory includes questions about dietary intake, weight change, and risk factors for nutritional deficiencies\nand a focused review of systems (see\nTable 2-1\n on p. \n11\n). A dietitian can obtain a more detailed dietary history. It usually includes a list of foods\neaten within the previous 24 h and a food questionnaire. A food diary may be used to record all foods\neaten. The weighed ad libitum diet, in which the patient weighs and writes down all foods consumed, is\nthe most accurate record.\nA complete physical examination, including measurement of height and weight and distribution of body\nfat, should be done. Body mass index (BMI)—weight(kg)/height(m)\n2\n, which adjusts weight for height (see\nTable 6-2\n on p. \n59\n), is more accurate than height and weight tables. There are standards for growth and\nweight gain in infants, children, and adolescents (see p. \n2756\n).\nDistribution of body fat is important. Disproportionate truncal obesity (ie, waist/hip ratio > 0.8) is\nassociated with cardiovascular and cerebrovascular disorders, hypertension, and diabetes mellitus more\noften than fat located elsewhere. Measuring waist circumference in patients with a BMI of < 35 helps\ndetermine whether they have truncal obesity and helps predict risk of diabetes, hypertension,\nhypercholesterolemia, and cardiovascular disorders. Risk is increased if waist circumference is > 102 cm\n(> 40 in) in men or > 88 cm (> 35 in) in women.\nNutrient-Drug Interactions\nNutrition can affect the body's response to drugs; conversely, drugs can affect the body's nutrition.\nFoods can enhance, delay, or decrease drug absorption. Foods impair absorption of many antibiotics.\nThey can alter metabolism of drugs; eg, high-protein diets can accelerate metabolism of certain drugs by\nstimulating \ncytochrome P-450. Eating grapefruit can inhibit cytochrome P-450 34A, slowing metabolism of\nsome drugs (eg, amiodarone, carbamazepine, cyclosporine, certain Ca channel blockers). Diets that alter\nthe bacterial flora may markedly affect the overall metabolism of certain drugs. Some foods affect the\nbody's response to drugs. For example, tyramine, a component of cheese and a potent vasoconstrictor,\ncan cause hypertensive crisis in some patients who take monoamine oxidase inhibitors and eat cheese.\nNutritional deficiencies can affect drug absorption and metabolism. Severe energy and protein\ndeficiencies reduce enzyme tissue concentrations and may impair the response to drugs by reducing\nabsorption or protein binding and causing liver dysfunction. Changes in the GI tract can impair absorption\nand affect the response to a drug. Deficiency of Ca, Mg, or zinc may impair drug metabolism. Vitamin C\ndeficiency decreases activity of drug-metabolizing enzymes, especially in the elderly.\nMany drugs affect appetite, food absorption, and tissue metabolism (see\nTable 1-6\n). Some drugs (eg, metoclopramide) increase GI motility, decreasing food absorption. Other\ndrugs (eg, opioids, anticholinergics) decrease GI motility. Some drugs are better tolerated if taken with\nfood.\nCertain drugs affect mineral metabolism. For example, diuretics, especially thiazides, and corticosteroids\ncan deplete body K, increasing susceptibility to digoxin-induced cardiac arrhythmias. Repeated use of\nlaxatives may deplete K. Cortisol, desoxycorticosterone, and aldosterone cause marked Na and water\nretention, at least temporarily; retention is much less with prednisone, prednisolone, and some other\ncorticosteroid analogs. Sulfonylureas and lithium can impair the uptake or release of iodine by the thyroid.\nOral contraceptives can lower blood zinc levels and increase copper levels. Certain antibiotics (eg,\ntetracyclines) reduce iron absorption, as can certain foods (eg, vegetables, tea, bran).\nCertain drugs affect vitamin absorption or metabolism. Ethanol impairs thiamin utilization, and isoniazid\ninterferes with niacin and pyridoxine metabolism. Ethanol and oral contraceptives inhibit folate (folic\nMetadata: {'source_doc_id': 'MedicalDiagnosisManuals/The_Merck_Manual_of_Diagnosis_and_Therapy_2011 - 19th Edn........pdf66', 'chunk_hash': '93707991382effcc93fab35fa1f5d31b947c90140b4ba79f1b5b263925e6dcaf', 'mtime': None, 'page_number': 66, 'stats': {'tiktokens': 1024, 'chars': 4452, 'lines': 63}, 'source': {'filename': 'MedicalDiagnosisManuals/The_Merck_Manual_of_Diagnosis_and_Therapy_2011 - 19th Edn........pdf', 'url': 'MedicalDiagnosisManuals/The_Merck_Manual_of_Diagnosis_and_Therapy_2011 - 19th Edn........pdf', 'mtime': 1745183731.0}}\n"
        }
      ],
      "execution_count": 36,
      "metadata": {
        "gather": {
          "logged": 1745198517136
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5: Creating the System and User Prompt Templates**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write you are code here\n",
        "# Define the system prompt for the Azure OpenAI model\n",
        "qna_system_message = \"\"\"\n",
        "You are an assistant to a doctor. Your task is to summarize and provide relevant information to doctor's question based on the provided context.\n",
        "\n",
        "User input will include the necessary context for you to answer their questions. This context will begin with the token: ###Context.\n",
        "The context contains references to specific portions of documents relevant to the user's query, along with page number from the report.\n",
        "The source for the context will begin with the token ###Page\n",
        "\n",
        "When crafting your response:\n",
        "1. Select only context relevant to answer the question.\n",
        "2. Include the source links in your response.\n",
        "3. User questions will begin with the token: ###Question.\n",
        "4. If the question is irrelevant or if the context is empty - \"Sorry, this is out of my knowledge base\"\n",
        "\n",
        "Please adhere to the following guidelines:\n",
        "- Your response should only be about the question asked and nothing else.\n",
        "- Answer only using the context provided.\n",
        "- Do not mention anything about the context in your final answer.\n",
        "- If the answer is not found in the context, it is very very important for you to respond with \"Sorry, this is out of my knowledge base\"\n",
        "- If NO CONTEXT is provided, it is very important for you to respond with \"Sorry, this is out of my knowledge base\"\n",
        "\n",
        "Here is an example of how to structure your response:\n",
        "\n",
        "Answer:\n",
        "[Answer]\n",
        "\n",
        "Page:\n",
        "[Page number]\n",
        "\"\"\"\n",
        "\n",
        "# Define the user message template\n",
        "qna_user_message_template = \"\"\"\n",
        "###Context\n",
        "Here are some documents and their page number that are relevant to the question mentioned below.\n",
        "{context}  \n",
        "\n",
        "###Question  \n",
        "{question}  \n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": 37,
      "metadata": {
        "gather": {
          "logged": 1745198556928
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Generating the Response**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the rquired packages\n",
        "%pip install openai==1.55.3 tiktoken==0.6 session-info --quiet"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\nazureml-rag 0.2.38 requires tiktoken<1.0,>=0.7, but you have tiktoken 0.6.0 which is incompatible.\r\nlangchain-openai 0.3.14 requires openai<2.0.0,>=1.68.2, but you have openai 1.55.3 which is incompatible.\r\nlangchain-openai 0.3.14 requires tiktoken<1,>=0.7, but you have tiktoken 0.6.0 which is incompatible.\u001b[0m\u001b[31m\r\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 38,
      "metadata": {
        "gather": {
          "logged": 1745198563975
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import json\n",
        "import tiktoken\n",
        "import pandas as pd\n",
        "from openai import AzureOpenAI"
      ],
      "outputs": [],
      "execution_count": 39,
      "metadata": {
        "gather": {
          "logged": 1745198657068
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Azure OpenAI credentials\n",
        "with open('config-copy.json', 'r') as az_creds:\n",
        "    data = az_creds.read()\n",
        "\n",
        "creds = json.loads(data)"
      ],
      "outputs": [],
      "execution_count": 40,
      "metadata": {
        "gather": {
          "logged": 1745198660149
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Azure OpenAI client\n",
        "client = AzureOpenAI(\n",
        "    azure_endpoint=creds[\"AZURE_OPENAI_ENDPOINT\"],\n",
        "    api_key=creds[\"AZURE_OPENAI_KEY\"],\n",
        "    api_version=creds[\"AZURE_OPENAI_APIVERSION\"]\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 41,
      "metadata": {
        "gather": {
          "logged": 1745198663106
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        " def generate_rag_response(user_input):\n",
        "    # Retrieve relevant document chunks\n",
        "    relevant_document_chunks = retriever.similarity_search(user_input, k=3)\n",
        "    context_list = [d.page_content for d in relevant_document_chunks]\n",
        "\n",
        "    # Combine document chunks into a single context\n",
        "    context_for_query = \". \".join(context_list)\n",
        "\n",
        "    # Compose the prompt\n",
        "    prompt = [\n",
        "        {'role': 'system', 'content': qna_system_message},\n",
        "        {'role': 'user', 'content': qna_user_message_template.format(\n",
        "            context=context_for_query,\n",
        "            question=user_input\n",
        "            )\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Generate the response using Azure OpenAI\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=creds[\"CHATGPT_MODEL\"],\n",
        "            messages=prompt,\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "        # Extract and print the model's response\n",
        "        response = response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        response = f'Sorry, I encountered the following error: \\n {e}'\n",
        "\n",
        "\n",
        "    print(response)"
      ],
      "outputs": [],
      "execution_count": 42,
      "metadata": {
        "gather": {
          "logged": 1745198665414
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 1: What is the protocol for managing sepsis in a critical care unit?"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"What care and protocol should be follow to manage sepsis in a critical care unit?\"    # Enter the question to be answered by the system here \n",
        "generate_rag_response(user_input)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Answer:\nIn a critical care unit, the management of sepsis includes the following protocols:\n\n1. **Prompt Empiric Therapy**: Initiate parenteral antibiotics immediately after obtaining specimens for Gram stain and culture. The choice of antibiotics should be based on the suspected source, clinical setting, and previous culture results.\n\n2. **Antibiotic Regimens**: For septic shock of unknown cause, consider using gentamicin or tobramycin with a 3rd-generation cephalosporin. If resistant staphylococci or enterococci are suspected, add vancomycin. Include drugs effective against anaerobes if there is an abdominal source.\n\n3. **Surgical Intervention**: Drain abscesses and surgically excise necrotic tissues to eliminate septic foci, as antibiotic therapy alone may not be sufficient.\n\n4. **Glucose Management**: Normalize blood glucose levels using a continuous IV insulin infusion to maintain glucose between 80 to 110 mg/dL.\n\n5. **Corticosteroid Therapy**: Administer replacement doses of corticosteroids, such as hydrocortisone and fludrocortisone, during hemodynamic instability.\n\n6. **Activated Protein C**: Consider using drotrecogin alfa for severe sepsis and septic shock in patients with a high risk of death, while assessing the risk of bleeding.\n\n7. **Monitoring**: Implement continuous monitoring of vital signs, fluid intake and output, and perform routine blood tests to detect problems early.\n\n8. **Supportive Care**: Provide adequate nutrition and prevent complications such as infections and stress ulcers.\n\nPage:\n2447, 2391\n"
        }
      ],
      "execution_count": 43,
      "metadata": {
        "gather": {
          "logged": 1745198672151
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"Please list common symptoms for appendicitis. Can we cure this via medicine? If not, then please describe as what surgical procedure should we follow to treat it.\"   # Enter the question to be answered by the system here \n",
        "generate_rag_response(user_input)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Common symptoms of appendicitis include:\n- Epigastric or periumbilical pain followed by nausea, vomiting, and anorexia.\n- Pain that shifts to the right lower quadrant after a few hours.\n- Pain increases with cough and motion.\n- Right lower quadrant direct and rebound tenderness at McBurney's point.\n- Additional signs such as Rovsing sign, psoas sign, and obturator sign.\n- Low-grade fever (rectal temperature 37.7 to 38.3° C [100 to 101° F]).\n\nAppendicitis cannot be cured via medicine alone. The treatment for acute appendicitis is surgical removal, which can be performed either through open or laparoscopic appendectomy. It is crucial to perform the surgery promptly, as treatment delay increases mortality.\n\nPage:\n164-165\n"
        }
      ],
      "execution_count": 44,
      "metadata": {
        "gather": {
          "logged": 1745198751454
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"Please list out effective treatments for addressing sudden patchy hair loss, commonly seen as localized blad spots. What could be possible causes behind it?\"   # Enter the question to be answered by the system here \n",
        "generate_rag_response(user_input)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Answer:\nEffective treatments for sudden patchy hair loss (alopecia areata) include:\n- Corticosteroids (intralesional or topical)\n- Topical anthralin\n- Minoxidil\n- Induction of allergic contact dermatitis using diphencyprone or squaric acid dibutylester\n\nPossible causes behind sudden patchy hair loss include:\n- Autoimmune disorders\n- Environmental triggers (such as infection or emotional stress)\n- Coexisting autoimmune conditions (e.g., vitiligo or thyroiditis)\n\nPage:\n849\n"
        }
      ],
      "execution_count": 45,
      "metadata": {
        "gather": {
          "logged": 1745198772756
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 4:  What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"Please list out recommended treatment for physicqal injury to patients brain tissue that resulted in temporary or permanent impairment of brain function.\"   # Enter the question to be answered by the system here \n",
        "generate_rag_response(user_input)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Answer:\nThe recommended treatment for traumatic brain injury (TBI) includes:\n\n1. Ensuring a reliable airway and maintaining adequate ventilation, oxygenation, and blood pressure.\n2. Surgery may be needed for severe injuries to:\n   - Place monitors to track and treat intracranial pressure.\n   - Decompress the brain if intracranial pressure is increased.\n   - Remove intracranial hematomas.\n3. In the first few days post-injury, focus on maintaining adequate brain perfusion and oxygenation, and preventing complications of altered sensorium.\n4. Rehabilitation may be required for many patients after the initial treatment.\n\nPage:\n3395-3400\n"
        }
      ],
      "execution_count": 46,
      "metadata": {
        "gather": {
          "logged": 1745198834134
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"Please list out necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\"   # Enter the question to be answered by the system here \n",
        "generate_rag_response(user_input)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Answer:\nFor a person who has fractured their leg during a hiking trip, the following precautions and treatment steps should be taken:\n\n1. **Immediate Care**:\n   - **Rest**: Prevent further injury by avoiding movement.\n   - **Ice**: Apply ice to minimize swelling and pain (15 to 20 minutes every 1-2 hours).\n   - **Compression**: Use an elastic bandage or splint to compress the injury.\n   - **Elevation**: Keep the injured leg elevated above heart level to reduce swelling.\n\n2. **Immobilization**:\n   - Use a splint to immobilize the leg and prevent further injury.\n   - A cast may be required for long-term immobilization depending on the severity of the fracture.\n\n3. **Medical Evaluation**:\n   - Seek medical attention for a thorough evaluation and treatment.\n   - X-rays may be necessary to assess the fracture.\n   - Treatment may involve reduction (realignment of the bone) and possibly surgical intervention if the fracture is severe.\n\n4. **Pain Management**:\n   - Pain can be managed with medications, typically opioids for severe pain.\n\n5. **Post-Treatment Care**:\n   - Follow instructions for cast care if applicable (keeping it dry, inspecting for skin irritation).\n   - Engage in physical therapy as recommended to regain strength and mobility.\n\n6. **Monitoring for Complications**:\n   - Watch for signs of complications such as increased pain, swelling, or changes in skin color, which may indicate issues like compartment syndrome.\n\nPage:\n3634, 3379, 3381\n"
        }
      ],
      "execution_count": 47,
      "metadata": {
        "gather": {
          "logged": 1745198896006
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Power Ahead!"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}